# Polygon.io Data Ingestion Pipeline

Efficient ingestion pipeline for historical Polygon.io market data, including CSV parsing, Parquet conversion, and optional data lake storage.

This repository contains a high-performance, modular pipeline for ingesting historical market data from [Polygon.io](https://polygon.io/). It is designed to process flat file dumps (e.g., `2023-06-05.csv.gz`), convert them into analysis-friendly formats like Parquet, and store them in a structured data lake.

## ğŸ“¦ Features

- ğŸ—ƒï¸ Batch ingestion of compressed CSV files
- ğŸ§± Conversion to columnar Parquet format
- ğŸ§ª Schema enforcement and file validation
- âš¡ Optional multi-threaded processing
- ğŸª£ Compatible with local and cloud (S3) storage
- ğŸ§© Modular design for extensibility (e.g., DuckDB, Snowflake, BigQuery)
- ğŸ§­ For use in quantitative research and trading systems

## ğŸ“‚ Example Directory Structure

polygonio-data-ingestion/
â”œâ”€â”€ data/                            # ğŸ”½ Input & Output Data Folder
â”‚   â”œâ”€â”€ raw/                         # Original compressed CSVs from Polygon.io
â”‚   â”‚   â”œâ”€â”€ trades/
â”‚   â”‚   â”‚   â”œâ”€â”€ 2023-06-01.csv.gz
â”‚   â”‚   â”‚   â””â”€â”€ 2023-06-02.csv.gz
â”‚   â”‚   â””â”€â”€ quotes/
â”‚   â”‚       â”œâ”€â”€ 2023-06-01.csv.gz
â”‚   â”‚       â””â”€â”€ 2023-06-02.csv.gz
â”‚   â””â”€â”€ parquet/                     # Transformed Parquet output
â”‚       â”œâ”€â”€ trades/
â”‚       â”‚   â””â”€â”€ 2023/06/2023-06-01.parquet
â”‚       â””â”€â”€ quotes/
â”‚           â””â”€â”€ 2023/06/2023-06-01.parquet

â”œâ”€â”€ ingest/                          # ğŸ›  Core Ingestion Logic
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ loader.py                    # Load raw files
â”‚   â”œâ”€â”€ parser.py                    # Parse and clean data
â”‚   â”œâ”€â”€ writer.py                    # Write to Parquet
â”‚   â””â”€â”€ pipeline.py                  # End-to-end ingestion flow

â”œâ”€â”€ config/                          # âš™ Config files
â”‚   â””â”€â”€ config.yaml                  # Source paths, output paths, schema, etc.

â”œâ”€â”€ scripts/                         # ğŸ“œ CLI or helper scripts
â”‚   â”œâ”€â”€ run_ingestion.py             # Example entrypoint for ingestion
â”‚   â””â”€â”€ cron_wrapper.sh              # (Optional) For scheduled execution

â”œâ”€â”€ notebooks/                       # ğŸ““ Optional analysis or demo notebooks
â”‚   â””â”€â”€ visualize_sample.ipynb

â”œâ”€â”€ tests/                           # âœ… Unit tests
â”‚   â”œâ”€â”€ test_loader.py
â”‚   â”œâ”€â”€ test_parser.py
â”‚   â””â”€â”€ test_writer.py

â”œâ”€â”€ .gitignore                       # Ignore data, logs, etc.
â”œâ”€â”€ LICENSE
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â””â”€â”€ setup.py                         # (Optional) To make package installable

## ğŸš€ Quick Start

```bash
# Clone the repository
git clone https://github.com/mengren1942/polygonio-data-ingestion.git
cd polygonio-data-ingestion

# (Optional) Create environment
conda create -n polygon-ingest python=3.11
conda activate polygon-ingest
pip install -r requirements.txt

# Run the ingestion script
# Usage (Linux/macOS):
# (optional) raise FD limit
ulimit -n 16384

python polygon_ingest_monthslice.py \
  --src minute_aggs_v1 \
  --out parquet_lake \
  --workers 16 \
  --chunk 5000000 \
  --watch ticker_lists/nasdaq100.json
