# Polygon.io Data Ingestion Pipeline

Efficient ingestion pipeline for historical Polygon.io market data, including CSV parsing, Parquet conversion, and optional data lake storage.

This repository contains a high-performance, modular pipeline for ingesting historical market data from [Polygon.io](https://polygon.io/). It is designed to process flat file dumps (e.g., `2023-06-05.csv.gz`), convert them into analysis-friendly formats like Parquet, and store them in a structured data lake.

## ğŸ“¦ Features

- ğŸ—ƒï¸ Batch ingestion of compressed CSV files
- ğŸ§± Conversion to columnar Parquet format
- ğŸ§ª Schema enforcement and file validation
- âš¡ Optional multi-threaded processing
- ğŸª£ Compatible with local and cloud (S3) storage
- ğŸ§© Modular design for extensibility (e.g., DuckDB, Snowflake, BigQuery)
- ğŸ§­ For use in quantitative research and trading systems

## ğŸ“‚ Example Directory Structure

polygonio-data-ingestion/
â”œâ”€â”€ data/                          # Input & Output Data Folder
â”‚   â”œâ”€â”€ raw/                       # Original compressed CSVs from Polygon.io
â”‚   â”‚   â”œâ”€â”€ trades/
â”‚   â”‚   â””â”€â”€ quotes/
â”‚   â””â”€â”€ parquet/                   # Transformed Parquet output
â”‚       â”œâ”€â”€ trades/
â”‚       â””â”€â”€ quotes/
â”œâ”€â”€ ingest/                        # Core ingestion logic
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ loader.py                  # Load raw files
â”‚   â”œâ”€â”€ parser.py                  # Parse and clean data
â”‚   â”œâ”€â”€ writer.py                  # Write to Parquet
â”‚   â””â”€â”€ pipeline.py                # End-to-end pipeline coordination
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.yaml                # Source paths, schema, etc.
â”œâ”€â”€ scripts/                       # CLI or automation scripts
â”‚   â”œâ”€â”€ run_ingestion.py
â”‚   â””â”€â”€ cron_wrapper.sh            # (Optional) for cron jobs
â”œâ”€â”€ notebooks/                     # Optional notebooks for demos
â”‚   â””â”€â”€ visualize_sample.ipynb
â”œâ”€â”€ tests/                         # Unit tests
â”‚   â”œâ”€â”€ test_loader.py
â”‚   â”œâ”€â”€ test_parser.py
â”‚   â””â”€â”€ test_writer.py
â”œâ”€â”€ .gitignore
â”œâ”€â”€ LICENSE
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â””â”€â”€ setup.py                       # (Optional) to make this installable as a package

## ğŸš€ Quick Start

```bash
# Clone the repository
git clone https://github.com/mengren1942/polygonio-data-ingestion.git
cd polygonio-data-ingestion

# (Optional) Create environment
conda create -n polygon-ingest python=3.11
conda activate polygon-ingest
pip install -r requirements.txt

# Run the ingestion script
# Usage (Linux/macOS):
# (optional) raise FD limit
ulimit -n 16384

python polygon_ingest_monthslice.py \
  --src minute_aggs_v1 \
  --out parquet_lake \
  --workers 16 \
  --chunk 5000000 \
  --watch ticker_lists/nasdaq100.json
